"""
Fine Tuning - Detailed Breakdown
============================================

[Optional: longer overview paragraph you can fill in later]
"""

import os
import sys
import subprocess
from pathlib import Path

TOPIC_NAME = "Fine Tuning_Detailed Breakdown"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# PATH TO THE PIPELINE SCRIPT
# Adjust this to match your actual project layout
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# This resolves relative to this file's location:
#   topics/08_a_FineTuning_FullFineTuning.py
#   Implementation/Full_Fine_Tuning_Implementation/scripts/Full_fine_tuning_main.py
_THIS_DIR = Path(__file__).resolve().parent
_PROJECT_ROOT = _THIS_DIR.parent
_SCRIPTS_DIR = _PROJECT_ROOT / "Implementation" / "Full_Fine_Tuning_Implementation" / "scripts"
_MAIN_SCRIPT = _SCRIPTS_DIR / "Full_fine_tuning_main.py"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# THEORY  (unchanged â€” keeping your existing content)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

THEORY = """

### Fine Tuning Detailed Breakdown

                                                                            FINE-TUNING METHODS HIERARCHY â€” LANDSCAPE VIEW

                                                â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

                                                                                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                                                            â”‚     FINE-TUNING      â”‚
                                                                                            â”‚  (Adapting a model   â”‚
                                                                                            â”‚   to a specific task)â”‚
                                                                                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                                                                       â”‚
                                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                  â”‚                                                                    â”‚                                                                    â”‚
                                  â–¼                                                                    â–¼                                                                    â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚       FULL FINE-TUNING        â”‚                              â”‚    PEFT (Parameter-Efficient           â”‚                             â”‚       ALIGNMENT TUNING              â”‚
                  â”‚                               â”‚                              â”‚    Fine-Tuning)                        â”‚                             â”‚    (Human Preference-Based)         â”‚
                  â”‚  â€¢ ALL params updated         â”‚                              â”‚                                        â”‚                             â”‚                                     â”‚
                  â”‚  â€¢ Best quality potential     â”‚                              â”‚  â€¢ Only a SUBSET of params updated     â”‚                             â”‚  â€¢ Aligns model behavior            â”‚
                  â”‚  â€¢ Highest cost (GPU/memory)  â”‚                              â”‚  â€¢ Lower cost (memory & compute)       â”‚                             â”‚    with human values                â”‚
                  â”‚  â€¢ Risk of catastrophic       â”‚                              â”‚  â€¢ Preserves pre-trained knowledge     â”‚                             â”‚  â€¢ Uses ranked preferences          â”‚
                  â”‚    forgetting                 â”‚                              â”‚  â€¢ Modular (swap adapters per task)    â”‚                             â”‚    or reward signals                â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚                                                                         â”‚                                                                     â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                         â”‚                                                                     â”‚
              â–¼               â–¼               â–¼                                                         â”‚                                                                     â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                  â”‚                                                                     â”‚
      â”‚  Standard    â”‚â”‚  Feature     â”‚â”‚  Gradual     â”‚                                                  â”‚                                                                     â”‚
      â”‚  Full FT     â”‚â”‚  Extraction  â”‚â”‚  Unfreezing  â”‚                                                  â”‚                                                                     â”‚
      â”‚              â”‚â”‚              â”‚â”‚              â”‚                                                  â”‚                                                                     â”‚
      â”‚ All layers   â”‚â”‚ Freeze base, â”‚â”‚ Unfreeze     â”‚                                                  â”‚                                                                     â”‚
      â”‚ unlocked     â”‚â”‚ train new    â”‚â”‚ layers one   â”‚                                                  â”‚                                                                     â”‚
      â”‚ from start   â”‚â”‚ head only    â”‚â”‚ by one       â”‚                                                  â”‚                                                                     â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                  â”‚                                                                     â”‚
                                                                                                        â”‚                                                                     â”‚
                                                                                                        â”‚                                                                     â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
                    â”‚                              â”‚                              â”‚                     â”‚                  â”‚                              â”‚                   â”‚
                    â–¼                              â–¼                              â–¼                     â”‚                  â–¼                              â–¼                   â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
        â”‚    ADDITIVE METHODS       â”‚ â”‚   REPARAMETERIZATION      â”‚ â”‚    SELECTIVE METHODS      â”‚       â”‚      â”‚     HYBRID METHODS        â”‚ â”‚     PROMPT METHODS        â”‚    â”‚
        â”‚                           â”‚ â”‚                           â”‚ â”‚                           â”‚       â”‚      â”‚                           â”‚ â”‚                           â”‚    â”‚
        â”‚  Add NEW parameters       â”‚ â”‚  Transform existing       â”‚ â”‚  Select WHICH existing    â”‚       â”‚      â”‚  Combine multiple PEFT    â”‚ â”‚  Learn soft prompts,      â”‚    â”‚
        â”‚  to the model while       â”‚ â”‚  params via low-rank      â”‚ â”‚  params to train and      â”‚       â”‚      â”‚  strategies (e.g.         â”‚ â”‚  NOT weights. Trainable   â”‚    â”‚
        â”‚  freezing originals       â”‚ â”‚  decomposition            â”‚ â”‚  freeze the rest          â”‚       â”‚      â”‚  quantization + adapters) â”‚ â”‚  tokens prepended to inputâ”‚    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
                      â”‚                             â”‚                             â”‚                     â”‚                    â”‚                              â”‚                 â”‚
                      â”‚                             â”‚                             â”‚                     â”‚                    â”‚                              â”‚                 â”‚
                      â–¼                             â–¼                             â–¼                     â”‚                    â–¼                              â–¼                 â”‚
                                                                                                       â”‚                                                                    â”‚
                 (See individual topic modules for each method)                                         â”‚                                                                    â”‚
                                                                                                       â”‚                                                                    â”‚
                                                                                                       â”‚                                                                    â”‚
                                                                                                       â”‚                                                                    â”‚
                                                                                                       â”‚                                                                    â”‚
                                                                                                       â”‚                                                                    â”‚
                                                                                                       â”‚                                                                    â”‚
                                                                                                       â”‚                                                                    â”‚
                                                                                                       â”‚                                                                    â”‚
            (Refer to the full version of this chart in the theory for complete details)                â”‚                                                                    â”‚

"""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# COMPLEXITY / COMPARISON TABLE
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

COMPLEXITY = """
| Aspect          | Detail          |
|-----------------|-----------------|
| Parameters      |                 |
| Training Time   |                 |
| Inference Time  |                 |
"""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# OPERATIONS â€” Code snippets (these still appear in the standard Operations tab)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

OPERATIONS = {
    "Full Pipeline Overview": {
        "description": "The Full Fine-Tuning pipeline: Token Check â†’ VRAM â†’ Data Prep â†’ Training â†’ Inference â†’ Compare",
        "runnable": False,
        "code": '''# Full Fine-Tuning Pipeline Steps
# ================================
# 1. Token Verification  â€” Validate HuggingFace credentials
# 2. VRAM Check          â€” Estimate GPU memory requirements
# 3. Data Preparation    â€” Download, format & tokenize dataset
# 4. Training            â€” Full fine-tuning (ALL parameters)
# 5. Inference           â€” Test your fine-tuned model
# 6. Compare             â€” Side-by-side: original vs fine-tuned
#
# Run from CLI:
#   python Full_fine_tuning_main.py                    # Interactive menu
#   python Full_fine_tuning_main.py --run all          # Full pipeline
#   python Full_fine_tuning_main.py --run train --yes  # Train, auto-confirm
#
# Or use the ğŸš€ Pipeline Runner tab to run from within Streamlit!
'''
    },

    "Training Configuration": {
        "description": "Key training hyperparameters for full fine-tuning (from training_config.yaml)",
        "runnable": False,
        "code": '''# training_config.yaml â€” Key Parameters
# ======================================
model_name: "unsloth/Llama-3.2-1B-Instruct"
dataset_name: "yahma/alpaca-cleaned"
max_seq_length: 512

# Batch & Accumulation
per_device_train_batch_size: 1       # Fits in VRAM
gradient_accumulation_steps: 8       # Effective batch = 1 Ã— 8 = 8

# Optimizer & Schedule
learning_rate: 2e-5
weight_decay: 0.01
warmup_ratio: 0.03
lr_scheduler_type: "cosine"

# Training Duration
num_train_epochs: 3                  # ~17,000 steps on 52K examples

# Precision & Memory
bf16: true
gradient_checkpointing: true         # Trades compute for VRAM savings

# Checkpointing
save_strategy: "steps"
save_steps: 500
save_total_limit: 2
'''
    },

    "VRAM Estimation Formula": {
        "description": "How GPU VRAM requirements are estimated for full fine-tuning",
        "runnable": False,
        "code": '''# VRAM Estimation for Full Fine-Tuning
# =====================================
# For a model with P parameters in bf16:
#
# Model Weights:     P Ã— 2 bytes  (bf16 = 2 bytes per param)
# Gradients:         P Ã— 2 bytes  (same dtype as weights)
# Optimizer (AdamW): P Ã— 8 bytes  (2 states Ã— 4 bytes each, fp32)
# Activations:       ~1-4 GB      (depends on seq_len, batch_size)
#
# Example: Llama-3.2-1B (1.24B params)
#   Weights:    1.24B Ã— 2 = 2.48 GB
#   Gradients:  1.24B Ã— 2 = 2.48 GB
#   Optimizer:  1.24B Ã— 8 = 9.92 GB
#   Activations: ~1.5 GB (with gradient checkpointing)
#   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#   TOTAL:      ~16.4 GB
#
# gradient_checkpointing=True reduces activation memory by ~60-70%
# at the cost of ~20-30% slower training (recomputes activations)
'''
    },
}


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# STREAMLIT PIPELINE RUNNER
# This function renders an interactive UI for running the fine-tuning pipeline
# directly from within the Streamlit app.
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def render_operations():
    """
    Custom Streamlit UI for running the Full Fine-Tuning pipeline.

    Called by app_Testing.py when this topic is selected, instead of
    the default code-snippet rendering.

    Features:
    - Editable config panel (model, batch size, learning rate, etc.)
    - Run individual pipeline steps via buttons
    - Real-time streaming output display
    - Step status tracking
    """
    import streamlit as st

    # â”€â”€â”€ Session State Initialization â”€â”€â”€
    if "fft_step_outputs" not in st.session_state:
        st.session_state.fft_step_outputs = {}
    if "fft_step_status" not in st.session_state:
        st.session_state.fft_step_status = {}
    if "fft_running" not in st.session_state:
        st.session_state.fft_running = False

    # â”€â”€â”€ Resolve Script Path â”€â”€â”€
    script_path = _MAIN_SCRIPT
    scripts_dir = _SCRIPTS_DIR

    if not script_path.exists():
        st.error(
            f"Pipeline script not found at:\n`{script_path}`\n\n"
            f"Please verify the path in `08_a_FineTuning_FullFineTuning.py` "
            f"(variables `_SCRIPTS_DIR` and `_MAIN_SCRIPT`)."
        )
        # Still show the standard operations as fallback
        _render_standard_operations(st)
        return

    # â”€â”€â”€ Layout â”€â”€â”€
    config_tab, runner_tab, code_tab = st.tabs([
        "âš™ï¸ Configuration",
        "ğŸš€ Pipeline Runner",
        "ğŸ“ Code Reference"
    ])

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # TAB 1: CONFIGURATION
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    with config_tab:
        st.markdown("### Training Configuration")
        st.caption("These values are sent to the pipeline. Edit before running.")

        col1, col2 = st.columns(2)

        with col1:
            st.markdown("**Model & Data**")
            model_name = st.selectbox(
                "Model",
                options=[
                    "unsloth/Llama-3.2-1B-Instruct",
                    "meta-llama/Llama-3.2-1B-Instruct",
                    "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                    "HuggingFaceTB/SmolLM2-360M-Instruct",
                    "openai-community/gpt2",
                ],
                index=0,
                key="fft_model_name",
            )
            dataset_name = st.text_input(
                "Dataset", value="yahma/alpaca-cleaned", key="fft_dataset"
            )
            max_seq_length = st.select_slider(
                "Max Sequence Length",
                options=[128, 256, 512, 1024, 2048],
                value=512,
                key="fft_seq_len",
            )

        with col2:
            st.markdown("**Training Hyperparameters**")
            batch_size = st.number_input(
                "Per-Device Batch Size", min_value=1, max_value=16, value=1,
                key="fft_batch_size",
            )
            grad_accum = st.number_input(
                "Gradient Accumulation Steps", min_value=1, max_value=64, value=8,
                key="fft_grad_accum",
            )
            num_epochs = st.number_input(
                "Epochs", min_value=1, max_value=10, value=3,
                key="fft_epochs",
            )
            learning_rate = st.select_slider(
                "Learning Rate",
                options=[1e-6, 5e-6, 1e-5, 2e-5, 5e-5, 1e-4, 2e-4],
                value=2e-5,
                format_func=lambda x: f"{x:.0e}",
                key="fft_lr",
            )

        col3, col4 = st.columns(2)

        with col3:
            st.markdown("**Precision & Memory**")
            use_bf16 = st.checkbox("Use bf16 (Brain Float 16)", value=True, key="fft_bf16")
            use_grad_ckpt = st.checkbox(
                "Gradient Checkpointing (saves VRAM)", value=True, key="fft_grad_ckpt"
            )
            lr_scheduler = st.selectbox(
                "LR Scheduler",
                options=["cosine", "linear", "constant", "constant_with_warmup"],
                index=0,
                key="fft_scheduler",
            )

        with col4:
            st.markdown("**Checkpointing & Logging**")
            logging_steps = st.number_input(
                "Logging Steps", min_value=1, max_value=100, value=10,
                key="fft_log_steps",
            )
            save_steps = st.number_input(
                "Save Checkpoint Every N Steps", min_value=50, max_value=2000, value=500,
                key="fft_save_steps",
            )
            eval_steps = st.number_input(
                "Eval Every N Steps", min_value=50, max_value=2000, value=200,
                key="fft_eval_steps",
            )

        # Show effective batch size
        effective_bs = batch_size * grad_accum
        st.info(f"**Effective batch size:** {batch_size} Ã— {grad_accum} = **{effective_bs}**")

        # Build config dict (used by the pipeline)
        config = {
            "model_name": model_name,
            "dataset_name": dataset_name,
            "max_seq_length": max_seq_length,
            "per_device_train_batch_size": batch_size,
            "per_device_eval_batch_size": 2,
            "gradient_accumulation_steps": grad_accum,
            "num_train_epochs": num_epochs,
            "learning_rate": learning_rate,
            "weight_decay": 0.01,
            "warmup_ratio": 0.03,
            "lr_scheduler_type": lr_scheduler,
            "bf16": use_bf16,
            "gradient_checkpointing": use_grad_ckpt,
            "output_dir": "./outputs/llama-3.2-1B-full-ft",
            "logging_steps": logging_steps,
            "eval_strategy": "steps",
            "eval_steps": eval_steps,
            "save_strategy": "steps",
            "save_steps": save_steps,
            "save_total_limit": 2,
            "seed": 42,
        }

        # Store config in session state for the runner tab
        st.session_state.fft_config = config

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # TAB 2: PIPELINE RUNNER
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    with runner_tab:
        st.markdown("### Full Fine-Tuning Pipeline")
        st.caption(
            "Run each step individually or the full pipeline. "
            "Output streams in real-time below each step."
        )

        # Pipeline steps definition
        steps = [
            ("token", "1. Verify HF Token", "Validates your HuggingFace credentials and model access"),
            ("vram", "2. Check VRAM", "Estimates GPU memory requirements for your config"),
            ("prepare", "3. Prepare Dataset", "Downloads, formats, and tokenizes the training data"),
            ("train", "4. Train Model", "Full fine-tuning â€” âš ï¸ Takes HOURS (see warning below)"),
            ("inference", "5. Test Inference", "Generate text with the fine-tuned model"),
            ("compare", "6. Compare Models", "Side-by-side comparison: original vs fine-tuned"),
        ]

        # â”€â”€ Training Warning â”€â”€
        with st.expander("âš ï¸ Training Time Warning", expanded=False):
            st.warning(
                "**Full fine-tuning is EXTREMELY time-consuming!**\n\n"
                "Estimated time:\n"
                "- RTX 3090 (24 GB): ~3-6 hours\n"
                "- RTX 4090 (24 GB): ~2-4 hours\n"
                "- A100 (40/80 GB): ~1-2 hours\n"
                "- CPU only: Days (not recommended)\n\n"
                "~17,000+ optimizer steps across 3 epochs over 52K examples. "
                "Do NOT close the browser during training."
            )

        st.markdown("---")

        # â”€â”€ Individual Step Runners â”€â”€
        for step_key, step_label, step_desc in steps:
            with st.container(border=True):
                col_info, col_btn = st.columns([3, 1])

                with col_info:
                    # Status indicator
                    status = st.session_state.fft_step_status.get(step_key, "pending")
                    status_icons = {
                        "pending": "â¬œ",
                        "running": "ğŸ”„",
                        "success": "âœ…",
                        "failed": "âŒ",
                    }
                    icon = status_icons.get(status, "â¬œ")
                    st.markdown(f"**{icon} {step_label}**")
                    st.caption(step_desc)

                with col_btn:
                    st.markdown("")  # vertical spacer
                    # Extra confirmation for training step
                    if step_key == "train":
                        confirm_train = st.checkbox(
                            "I understand this takes hours",
                            key="fft_confirm_train",
                        )
                        run_disabled = not confirm_train
                    else:
                        run_disabled = False

                    if st.button(
                            f"Run",
                            key=f"fft_run_{step_key}",
                            use_container_width=True,
                            disabled=run_disabled,
                            type="primary" if step_key == "train" else "secondary",
                    ):
                        _execute_step(st, step_key, step_label, script_path, scripts_dir)

                # Show output if available
                if step_key in st.session_state.fft_step_outputs:
                    output = st.session_state.fft_step_outputs[step_key]
                    with st.expander(f"ğŸ“‹ Output from {step_label}", expanded=True):
                        st.code(output, language="text")

        # â”€â”€ Full Pipeline Button â”€â”€
        st.markdown("---")
        st.markdown("### Run Full Pipeline")
        st.caption("Runs steps 1 â†’ 6 sequentially. Each step must succeed before the next starts.")

        confirm_full = st.checkbox(
            "I understand this will take several hours and I've configured everything above",
            key="fft_confirm_full",
        )

        if st.button(
                "ğŸš€ Run Full Pipeline",
                key="fft_run_all",
                disabled=not confirm_full,
                type="primary",
                use_container_width=True,
        ):
            _execute_full_pipeline(st, steps, script_path, scripts_dir)

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # TAB 3: CODE REFERENCE (Standard operations display)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    with code_tab:
        _render_standard_operations(st)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# HELPER: Execute a single pipeline step via subprocess
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _execute_step(st, step_key, step_label, script_path, scripts_dir):
    """
    Run a single pipeline step as a subprocess and stream output to Streamlit.

    Uses subprocess.Popen to launch:
        python Full_fine_tuning_main.py --run <step_key> --yes

    Output is captured line-by-line and displayed in real-time.
    """
    st.session_state.fft_step_status[step_key] = "running"

    # Build command
    cmd = [
        sys.executable,
        str(script_path),
        "--run", step_key,
        "--yes",  # Auto-confirm prompts (non-interactive mode)
    ]

    # For inference, add a default prompt
    if step_key == "inference":
        cmd.extend(["--prompt", "What is machine learning? Explain in 2 sentences."])

    output_lines = []
    output_placeholder = st.empty()

    try:
        output_placeholder.info(f"ğŸ”„ Running {step_label}...")

        process = subprocess.Popen(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            text=True,
            bufsize=1,  # Line-buffered
            cwd=str(scripts_dir),  # So local imports (check_vram, etc.) resolve
            env={**os.environ, "PYTHONUNBUFFERED": "1"},  # Force unbuffered output
        )

        # Stream output line-by-line
        for line in process.stdout:
            # Strip ANSI escape codes for clean display
            clean_line = _strip_ansi(line)
            output_lines.append(clean_line)
            # Update the display with accumulated output
            output_placeholder.code("".join(output_lines), language="text")

        process.wait()

        if process.returncode == 0:
            st.session_state.fft_step_status[step_key] = "success"
            output_lines.append(f"\n{'=' * 50}\nâœ… {step_label} completed successfully.\n")
        else:
            st.session_state.fft_step_status[step_key] = "failed"
            output_lines.append(
                f"\n{'=' * 50}\nâŒ {step_label} failed (exit code {process.returncode}).\n"
            )

    except FileNotFoundError:
        st.session_state.fft_step_status[step_key] = "failed"
        output_lines.append(f"âŒ Could not find Python or script at:\n  {script_path}\n")
    except Exception as e:
        st.session_state.fft_step_status[step_key] = "failed"
        output_lines.append(f"âŒ Error: {e}\n")

    # Store final output
    final_output = "".join(output_lines)
    st.session_state.fft_step_outputs[step_key] = final_output
    output_placeholder.code(final_output, language="text")


def _execute_full_pipeline(st, steps, script_path, scripts_dir):
    """Run all pipeline steps sequentially, stopping on failure."""
    progress_bar = st.progress(0, text="Starting pipeline...")
    total_steps = len(steps)

    for i, (step_key, step_label, _) in enumerate(steps):
        progress_bar.progress(
            (i) / total_steps,
            text=f"Running {step_label} ({i + 1}/{total_steps})..."
        )

        _execute_step(st, step_key, step_label, script_path, scripts_dir)

        if st.session_state.fft_step_status.get(step_key) == "failed":
            progress_bar.progress(
                (i + 1) / total_steps,
                text=f"âŒ Pipeline stopped at {step_label}"
            )
            st.error(f"Pipeline stopped: {step_label} failed. Fix the issue and retry.")
            return

    progress_bar.progress(1.0, text="âœ… Full pipeline completed!")
    st.success("All pipeline steps completed successfully!")
    st.balloons()


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# HELPER: Render standard OPERATIONS dict (fallback / code reference tab)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _render_standard_operations(st):
    """Render the OPERATIONS dict in standard expander format."""
    for op_name, op_data in OPERATIONS.items():
        with st.expander(f"â–¶ï¸ {op_name}", expanded=False):
            st.markdown(f"**Description:** {op_data['description']}")
            st.markdown("---")
            st.code(op_data["code"], language="python")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# HELPER: Strip ANSI escape codes from terminal output
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _strip_ansi(text):
    """Remove ANSI color/formatting codes from text."""
    import re
    ansi_escape = re.compile(r'\x1b\[[0-9;]*m')
    return ansi_escape.sub('', text)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CONTENT EXPORT
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def get_content():
    """Return all content for this topic module."""
    return {
        "theory": THEORY,
        "complexity": COMPLEXITY,
        "operations": OPERATIONS,
        "render_operations": render_operations,  # Custom Streamlit UI for pipeline
    }
