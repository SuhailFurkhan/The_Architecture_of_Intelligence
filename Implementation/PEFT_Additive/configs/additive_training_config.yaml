# ============================================================
# Additive PEFT Configuration — Bottleneck Adapters & (IA)³
# ============================================================
#
# ┌──────────────────────────────────────────────────────────────────┐
# │  ADDITIVE vs REPARAMETERIZATION — What is Different?             │
# │                                                                  │
# │  LoRA (Reparameterization):                                      │
# │    • Adds parallel bypass matrices alongside existing weights    │
# │    • Purely linear (no activation function)                      │
# │    • Merges into base model — zero inference overhead            │
# │                                                                  │
# │  Bottleneck Adapters (Additive):                                 │
# │    • Inserts NEW sequential modules BETWEEN existing layers      │
# │    • Has a non-linearity (GELU/ReLU) inside the bottleneck       │
# │    • CANNOT merge — permanent inference overhead                 │
# │    • More expressive for complex domain shifts                   │
# │                                                                  │
# │  (IA)³ (Additive):                                               │
# │    • Learns tiny scaling vectors applied to existing activations │
# │    • Element-wise multiply — linear, so CAN merge                │
# │    • Fewest parameters of ANY PEFT method (~0.01%)               │
# │    • Good for lightweight task steering                          │
# └──────────────────────────────────────────────────────────────────┘

# --- Model ---
model_name: "unsloth/Llama-3.2-1B-Instruct"

# ============================================================
# Method Selection
# ============================================================
# Choose which additive PEFT method to use:
#   "bottleneck"  — Bottleneck Adapters (Houlsby 2019 / Pfeiffer 2020)
#   "ia3"         — (IA)³ (Liu et al. 2022)

adapter_method: "bottleneck"    # change to "ia3" to use (IA)³ instead

# ============================================================
# Bottleneck Adapter Configuration
# ============================================================
#
# Architecture:
#   h' = h  +  W_up · GELU( W_down · h  +  b_down )  +  b_up
#
# W_down: [hidden_dim → bottleneck_dim]   — compress
# W_up:   [bottleneck_dim → hidden_dim]   — expand
# Non-linearity: GELU (default) or ReLU
# Residual: always added (ensures identity at init when W_up=0)
#
# Initialization:
#   W_down: random (Gaussian, small scale)
#   W_up:   ZEROS  → h' = h + 0 = h at step 0 (identity)

bottleneck_dim: 64              # Size of compressed bottleneck.
                                # Smaller = fewer params, less expressive.
                                # Rule of thumb:
                                #   bottleneck_dim = hidden_dim / reduction_factor
                                #   Common reduction factors: 16, 32, 64
                                #   hidden_dim=2048, factor=32 → bottleneck=64
                                #
                                # Effect on params (1B model, 16 layers):
                                #   dim=8:   ~1.1M params (0.09%)
                                #   dim=16:  ~2.1M params (0.17%)
                                #   dim=32:  ~4.3M params (0.34%)
                                #   dim=64:  ~8.5M params (0.68%)  ← default
                                #   dim=128: ~16.8M params (1.36%)

adapter_non_linearity: "gelu"   # Activation function inside bottleneck.
                                # "gelu" — smoother, better for most tasks
                                # "relu" — simpler, slightly less expressive

adapter_placement: "after_ffn"  # Where to insert the adapter in each layer.
                                # "after_ffn"          — Pfeiffer (2020), 1 per layer
                                #                         MORE EFFICIENT
                                # "after_attn_and_ffn" — Houlsby (2019), 2 per layer
                                #                         MORE EXPRESSIVE

# ============================================================
# (IA)³ Configuration
# ============================================================
#
# Architecture:
#   K  = (l_k  ⊙ W_k) · h      ← rescale key projections
#   V  = (l_v  ⊙ W_v) · h      ← rescale value projections
#   FF = W_down · (l_ff ⊙ GELU(gate)) × up   ← rescale FFN gate activations
#
# l_k, l_v, l_ff are learned vectors.
# Initialization: all ones → identity (no change at step 0)
#
# Parameter count (1B model, 16 layers, d=2048, kv_d=256, ffn=8192):
#   l_k:  16 × 256  =   4,096 params
#   l_v:  16 × 256  =   4,096 params
#   l_ff: 16 × 8192 = 131,072 params
#   TOTAL:            139,264 params  (0.011% of model!)

ia3_target_modules:             # Which activations to rescale.
  - "k_proj"                    # Key projection scaling (l_k)
  - "v_proj"                    # Value projection scaling (l_v)
  - "down_proj"                 # FFN gate scaling (l_ff)
                                # Note: "down_proj" is used as proxy for
                                # the FFN gate in HuggingFace PEFT's (IA)³

ia3_feedforward_modules:        # Which of target_modules are in the FFN
  - "down_proj"                 # (needed by PEFT to distinguish attn vs FFN)

# ============================================================
# Dataset
# ============================================================
dataset_name  : "yahma/alpaca-cleaned"
max_seq_length: 512
train_split   : "train[:90%]"
eval_split    : "train[90%:]"

# ============================================================
# Training Hyperparameters
# ============================================================
#
# LEARNING RATE NOTES:
#   Bottleneck Adapters: 1e-4 to 1e-3  (moderate — new modules with good init)
#   (IA)³:              1e-3 to 3e-3  (higher OK — very few params,
#                                      less risk of overshooting)
#   Full Fine-Tuning:   2e-5            (much lower — all params exposed)

num_train_epochs: 3
per_device_train_batch_size: 4
per_device_eval_batch_size: 4
gradient_accumulation_steps: 4          # Effective batch = 4 × 4 = 16
learning_rate: 1.0e-4                   # Tune based on method (see above)
weight_decay: 0.01
warmup_ratio: 0.03
lr_scheduler_type: "cosine"

# --- Memory ---
bf16: true
gradient_checkpointing: true
optim: "adamw_torch_fused"

# --- Logging ---
output_dir: "./outputs/llama-3.2-1B-additive"
logging_steps: 10
eval_strategy: "steps"
eval_steps: 200
save_strategy: "steps"
save_steps: 400
save_total_limit: 2

# --- Other ---
seed: 42
dataloader_num_workers: 0
report_to: "tensorboard"
save_dataset_locally: false

# ============================================================
# Save Options
# ============================================================
# For (IA)³: merge_before_save=true is recommended (linear, free to merge)
# For Bottleneck: merge_before_save is NOT supported (non-linear)

merge_before_save: false        # (IA)³ only — merge l vectors into weights
                                # before saving for zero-overhead inference
